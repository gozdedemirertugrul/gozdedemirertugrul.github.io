<!doctype html>
<html lang="en">

<head>
    <!-- Title and Favicon -->
    <title>Gözde Demir Ertuğrul</title>
    <link rel="icon" href="/images/base_site_images/logo.webp" type="image/x-icon" />

    <!-- Preload Critical Resources -->
    <link rel="preconnect" href="https://cdnjs.cloudflare.com">
    <link rel="preload" href="/css/base-site.css" as="style">
    <link rel="preload" href="/css/tooplate-style.css" as="style">

    <!-- Stylesheets -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha512-..." crossorigin="anonymous"
        referrerpolicy="no-referrer">
    <link rel="stylesheet" href="/css/base-site.css">
    <link rel="stylesheet" href="/css/tooplate-style.css">


</head>

<body>
    <div id="loader" class="loader-light">
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
    </div>

    <nav class="navbar navbar-expand-sm navbar-light bg-light">
        <div class="container-fluid">
          <div class="collapse navbar-collapse" id="navbarNav">
            <ul class="navbar-nav mx-auto">
              <!-- About -->
              <li class="nav-item">
                <a href="/index.html" class="nav-link">Home Page</a>
              </li>
      
              <!-- Dropdown (hover ile açılır) -->
              <li class="nav-item dropdown">
                <a class="nav-link dropdown-toggle" href="#" id="dropdownMenuLink" role="button"
                   data-bs-toggle="dropdown" aria-expanded="false">
                  Blog
                </a>
                <ul class="dropdown-menu" aria-labelledby="dropdownMenuLink">
                  <li><a class="dropdown-item" href="/pages/blogs/reliability.html">Reliability</a></li>
                  <li><a class="dropdown-item" href="/pages/blogs/validity.html">Validity</a></li>
                </ul>
              </li>
      
              <!-- Contact -->
              <li class="nav-item">
                <a href="/index.html#contact" class="nav-link">Contact</a>
              </li>
            </ul>
      
          </div>
        </div>
      </nav>


    <!-- Blog -->
    <section class="full-screen" id="project">
        <div class="container row">
            <img src="/images/blogs/validity/Validity.jpeg" class="img-fluid blog-header-image">
            <div class="col-lg-12 mx-auto">
                <h2>Validity</h2>
                <p><small>April 1, 2025</small></p>

                <p>
                <a href="#B1"><b>1.   Exploring the Concept of Validity in Testing: A Journey Through Its Historical Roots</b></a></br>
                <a href="#B1.1">&nbsp;&nbsp;&nbsp;&nbsp;1.1.	Between 1918 and 1939</a></br>
                <a href="#B1.2">&nbsp;&nbsp;&nbsp;&nbsp;1.2.	Between the 1930s and late 1960s</a></br>
                <a href="#B1.3">&nbsp;&nbsp;&nbsp;&nbsp;1.3.	Between 1960s and the 2000s </a></br>
                <a href="#B2"><b>2.	Innovative Methods in Explanation-centered Validity</b></a></br>
                <a href="#B3"><b>3.	Exploring Key Types of Test Validity: Content, Criterion, and Construct</b></a></br>
                <a href="#B3.1">&nbsp;&nbsp;&nbsp;&nbsp;3.1.	Content Validity </a></br>
                <a href="#B3.2">&nbsp;&nbsp;&nbsp;&nbsp;3.2.	Criterion-Related Validity </a></br>
                <a href="#B3.3">&nbsp;&nbsp;&nbsp;&nbsp;3.3.	Construct Validity </a></br>
                <a href="#BC"><b>Conclusion</b></a></br>
                <a href="#BR"><b>References</b></a></br>
                </p>
                
       

                <p>Hello all! Today, we will mention to test validity, are you ready to go? Ok, let’s start. The meaning of test validity has changed from the early 1900s to the early 2000s. In recent decades, the use of tests and evaluations in education has grown across the world. Large-scale tests, long-term assessments, individual evaluations, and surveys became more common during the rise of a global testing and assessment economy. However, this increase did not solve all the problems in the field. Some challenges and concerns still remain in how assessments are used (Zumbo, 2019). From here on, I would like to go deeper by dividing it into sections. </p>
                <p>In this writing, I will first explain the general idea of validity and its historical development. Then, I will describe the types of validity and focus on content validity, including the concept of item-objective congruence. Hope you enjoy reading! :)</p>
                <p>In this section, I would like to present what is meant by “validity” in the context of educational and psychological measurement by tracing how validity theory has evolved from the early 1900s to the present. First, the term “validity theory” will be used consistently throughout this article (2023) to avoid any possible ambiguity. In addition, Zumbo (2007a, 2009) emphasizes the need to make a clear distinction between the concepts of “validity” and “validity” from the outset. In educational and psychological measurement, validity refers to the nature or meaning of the interpretations made from test scores. Validity represents the extent to which a test measures what it purports to measure. In contrast, validity is the process used to collect and evaluate evidence to support these interpretations. While validity is a theoretical concept, validity involves the practical steps of data collection, analysis, and reasoning to justify the use of a test. As Zumbo (2007a, 2009) emphasizes, it is essential to clearly define what is meant by validity at the outset of any study, as confusion between validity and validation can lead to conceptual and methodological problems. Without a clear understanding of validity, validation efforts can become overly focused on technical procedures without addressing the fundamental meaning of what is being measured. </p>
                <p>Overall, in this article, I aim to introduce the concept of validity in educational and psychological measurement by explaining what validity means and how validity theory has evolved over time. It is important to make a clear distinction between validity and validation from the outset, as this distinction is often misunderstood. Validity refers to the degree to which interpretations made from test scores are appropriate, meaningful, and supported by evidence. Validation, on the other hand, is the process of collecting, analyzing, and evaluating evidence to justify these interpretations. Following Zumbo’s (2007, 2009, 2023) emphasis, this article will use the term validity theory consistently to explore the theoretical underpinnings of test interpretation and avoid its confusion with validation, which refers to the practical efforts that support it. Understanding this distinction is important to avoid conceptual and methodological errors in test development and use.</p>
                
                <b id="B1">1.   Exploring the Concept of Validity in Testing: A Journey Through Its Historical Roots</b>
                <p>In the early 1900s, the idea of validity started to appear in discussions about educational tests, even though there was no exact meaning defined back then. Instead of a formal definition, people talked about what makes a test good or meaningful. For example, Courtis (1921) believed the real problem was understanding what a test truly measures. Around the same time, Buckingham (1921) said that a test in intelligence should show how well someone can learn. These early ideas tell us three things: first, people thought validity belonged to the test itself, not the score; second, they didn’t explain a clear way to check for validity; and third, they mentioned using numbers and statistics to help, but without clear rules. Even today, these simple ideas still shape how we think about validity, as seen in later work by Borsboom and his team (2004, 2009).</p>

                <b id="B1.1">1.1.	Between 1918 and 1939</b>
                <p>From 1918 to 1939, behaviorism became the leading view in North American psychology and had a big impact on how tests were made and used. This way of thinking said that only visible and measurable actions should be studied; therefore it left out inner thoughts or feelings, saying they weren’t scientific enough (Hull, 1935; Watson, 1913). Because of this, during that time, tests were designed mostly to guess how someone might perform in the future, like in school or at work.</p>
                <p>In this period, people saw test scores as a way to predict future results, and validity was explained simply as how strongly test scores matched with another outside result (like school grades or job success) (Angoff, 1988; Bingham, 1937; Shear & Zumbo, 2014). This idea of validity focused on prediction, and test results were treated like quick indicators of future ability. So, proving that a test was valid mostly meant checking if it had strong statistical links with other known outcomes.</p>
                
                <b id="B1.2">1.2.	 Between the 1930s and late 1960s</b>
                <p>Between the 1930s and late 1960s, Hubley and Zumbo (1996) say that the idea of test validity went through a very active and exciting time. Many different opinions about what validity means started to appear, and experts talked a lot about them. This happened because people in the social and behavioral sciences wanted their fields to be more accepted as scientific. As a result, measuring things correctly became very important, and the topic of validity got more attention. Old views, like “a test is valid if it measures what it’s supposed to” or “if it can guess future performance,” were still popular. Anastasi (1950), for example, believed that a test is only useful if it matches a specific result. But Anastasi (1950) also said that things like someone’s mood or the testing situation could change the scores. Anastasi (1950) believed we should look at real-life actions, not just theory, when we study test results.</p>
                <p>Later, people started to see problems with the prediction-based idea of validity. Some researchers said that we can’t always use predictions to measure psychological things. Guilford (1946) said a test might be valid if it relates strongly to anything, even if it’s not what was originally planned. This allowed for more ways to think about validity. Around the same time, Rulon (1946) and Lennon (1956) talked about content validity. They said that if a test’s content matches what is taught in class, it can already be seen as valid. As more types of validity were named, test makers started picking the easiest or most useful kind of evidence. Hubley and Zumbo (1996) warned that while this was practical, it could also be confusing. Millman (1979) later said that we should choose how to test validity based on what the scores will be used for. This way of thinking helped change how people understood validity after the 1960s.</p>
                
                <b id="B1.3">1.3.	Between 1960s and the 2000s </b>
                <p>In the 1950s, the APA created four types of validity to help explain how tests should be checked. These were: content validity, predictive validity, concurrent validity, and construct validity. Among them, construct validity became the most important, but also the hardest to explain. Cronbach and Meehl (1955) said that a “construct” is a concept or idea that a test tries to measure. To trust a test, we must prove that the test scores really show this idea.</p>
                <p>Later, in 1957, Loevinger said that construct validity should be the center of all validity types. Loevinger (1957) thought other types were too simple and didn’t have strong theory behind them. In the 1970s and 1980s, Messick (1975) helped grow this idea. Messick (1975) said that we should not focus on the test alone, but also on what the test results mean. Messick (1975) added that we must think about how test results are used in the real world, because they can affect people’s lives.</p>
                <p>In the years that followed, many researchers shared new ideas. Embretson (1983, 2007) said that we should look at how test items match with mental abilities and theories. Haig (1999, 2014) explained that validity is about finding the best reason for why people get certain scores. Zumbo (2005, 2007, 2009) agreed, saying that we should ask what causes score differences. Kane (1992, 2006) added that if we use test scores to make decisions, we must give clear and strong reasons why.</p>
                <p>After 2000, even more ideas came out. Schaffner (2020) said that construct validity is about checking if the theory behind a test is getting stronger over time. Borsboom and his team (2004, 2009) had a stricter view. They said a test is only valid if what it says it measures is real and actually affects the scores. This showed a big change from just listing types of validity to building a deeper understanding.</p>
                <p>Zumbo (2023) went one step further. Zumbo (2023) said test scores are not fixed or simple. They change depending on the person, situation, and other outside factors. Zumbo (2023) said we should see scores as part of a bigger picture, not just numbers. He used ideas from math and older theories to explain this.</p>
                <p>Zumbo (2023) also believed that things like culture, life experience, and the testing environment all shape how people answer test questions. He used ideas from philosophy to say that scores mean different things in different situations. So, validity is not just about correct measurement, but about understanding what test scores really mean and how they are used (DeVellis and Thorpe, 2021).</p>
                <p>In the end, Zumbo (2023)  saw validity as a smart and human-centered process. Instead of just checking boxes, we should ask what the score tells us, who it affects, and how fair it is.</p>


                <b id="B2">2.	Innovative Methods in Explanation-centered Validity</b>
                <p>In Zumbo’s article (2023), Zumbo talked about how our understanding of Differential Item Functioning (DIF) has grown over time. Before, people saw DIF as only a small statistical issue. But in 2007, Zumbo introduced the idea of “Third Generation DIF.” Zumbo (2007a) said that instead of seeing DIF as a problem, we should see it as a way to better understand how different people answer test questions. Zumbo (2007a) pointed out that test responses can be influenced by many things, like culture, emotions, and social background. This matches his larger idea that test validity is not just about numbers but also about explanations (Zumbo, 2007a).</p>
                <p>Later, in 2015, Zumbo created something called the “ecological model of item response.” This model says that people’s answers are shaped by their surroundings, such as their classroom, community, and country. He used Bronfenbrenner’s ecological theory to explain this idea. It shows us that we need to look at the whole environment when thinking about how valid a test really is. Simple scoring is not enough, we must think about the human side, too.</p>
                <p>Zumbo (2007a) also described validation like playing jazz. Jazz is free, creative, and flexible, just like validation should be. Maddox and Zumbo (2017) said that test validation needs to consider how people really behave in testing situations. What happens around the person taking the test is important.</p>
                <p>Overall, Zumbo (2023) believes that validatity should be flexible, people-focused, and connected to real life. This is similar to what Cronbach and Meehl (1955) said long ago: a test must reflect real psychological traits, not just test scores. Zumbo adds to their work by including fairness, culture, and the test-taker’s world.</p>
                <b id="B3">3.	Exploring Key Types of Test Validity: Content, Criterion, and Construct</b>
                <p>In this part of my writing, I would like to talk about item-objective congruence (IOC) for items that measure more than one thing. Turner and Carlson (2003) talked about this idea, and they said that IOC was first created by Rovinelli and Hambleton (1977). IOC is used when tests are being made, to check if each question matches the goal of the test. It helps to understand content validity better.</p>
                <p>Therefore, while I talk about IOC, I will also explain what validity and content validity mean. After that, I plan to end this writing by writing about two other types of validity: criterion-related validity and construct validity.</p>
                <b id="B3.1">3.1.	Content Validity </b>
                <p>First, I will present validity definition under this heading, and then content validity. </p>
                <p>Validity refers to how accurately a test or instrument measures what it is intended to measure (Turner & Carlson, 2003). In other words, validity is about how well a test measures what it is supposed to measure (Turner & Carlson, 2003). In the article by Turner and Carlson (2003), the main focus is on content validity, which checks whether the test items are truly representing the knowledge, skills, or behaviors defined in the test objectives. Content validity is crucial during the test development process, as poor alignment between items and objectives can weaken the value of the overall assessment.</p>
                <p>Turner & Carlson (2003), discusses a method called the Index of Item-Objective Congruence (IOC), originally proposed by Rovinelli and Hambleton (1977). This index is used to evaluate how well each test item matches its intended objective. Experts review each item and assign scores: 1 if the item clearly matches the objective, 0 if unsure, and –1 if it clearly does not match. These scores are then used to calculate a value between –1 and +1, where higher values show stronger agreement among the experts that the item fits the defined goal (Turner & Carlson, 2003).</p>
                <p>However, the original IOC formula only applies to unidimensional items, meaning items that are supposed to measure just one objective. In practice, many items are designed to assess more than one skill or concept. To handle such cases, Turner and Carlson proposed an Adjusted Index of Item-Objective Congruence. This version allows experts to evaluate items that are intentionally multidimensional, offering a more flexible way to assess the match between items and multiple valid objectives.</p>
                <p>To calculate the adjusted IOC for multidimensional items, the authors presented a new formula that considers both valid and invalid objectives. Crocker and Algina (1986, p. 221) provided a simplified version of the formula in their Introduction to Classical and Modern Test Theory text:</p>
                <p>
                    \[
                    I'_{ik} = \frac{(N)\mu_K - (N - p)\mu_l}{2N - p}
                    \]
                </p>
                <p style="text-align: right">Eq. 1</p>
                <p>In this equation,  <br/>
                \(I'_{ik}\) is the adjusted index for item i on a set of objectives \( k \),  <br/>
                \( N \) is the total number of objectives,<br/>
                \( p \) is the number of valid objectives,<br/>
                \( \mu_K \) is the average expert rating on valid objectives, and<br/>
                \( \mu_l \) is the average rating on invalid objectives. 
                </p>

                <p>This formula allows for a full range of expert feedback while still producing a clear numeric result between –1 and 1 (Turner & Carlson, 2003, p. 169).</p>
                <p>In general, both the first and adjusted IOC formulas give test makers a clear method to check content validity. The original version is useful when each test item only matches one goal. But the new version is better when items are made to match more than one goal. These methods help test developers make sure their items really match what they want to measure. This way, tests become more correct and useful (Turner & Carlson, 2003).</p>
                <b id="B3.2">3.2.	Criterion-Related Validity</b>
                <p>Secondly, under this heading I will present criterion-related validity.</p>
                <p>In Crocker and Algina’ study (1986), criterion-related validity is about how well a test can guess or show how someone will do in real life, like at work or in school. Sometimes, we can't measure success directly, so we use test scores to see how someone might do in the future. This kind of validity is very useful when tests are used for decisions like hiring or choosing students (Mehrens & Lehmann, 1984).</p>
                <p>According to Crocker and Algina (1986), there are two main types: predictive and concurrent. Predictive validity means the test can show future success; for example, if a student’s exam score tells us how well they will do in college later. Concurrent validity means the test matches current performance; for example, a flying test and the pilot’s real flying skills at the same time (Mehrens & Lehmann, 1984).</p>
                <p>To check this validity, researchers first decide what result they want to predict (Crocker and Algina,1986). Then they give the test, collect real-world data later, and see if the test and the result match. Problems can happen if the sample is too small or if outside factors affect the scores. Also, weak reliability in the test or outcome can make the validity seem lower (Mehrens & Lehmann, 1984).</p>
                <p>According to Crocker and Algina (1986), to get better understanding, researchers use tools like expectancy tables, error estimates, and the coefficient of determination. These tools help show how strong the link is between the test and real performance. This makes test results more useful for making smart decisions, especially in school or work settings (Mehrens & Lehmann, 1984).</p>

                <b id="B3.3">3.3.	Construct Validity</b>
                <p>Lastly, under this heading I will present construct validity</p>
                <p>Construct validity is about whether a test really measures the mental idea it is supposed to. A "construct" is something we can't see directly, like intelligence, stress, or being outgoing (Crocker & Algina, 1986). To use a construct in a useful way, we first need to explain it clearly. Then, we must show how it is measured and how it connects with other things in life or in theory (Mehrens & Lehmann, 1984). If we don’t do this, the test may not give us helpful results.</p>
                <p>According to Crocker and Algina (1986), researchers take several steps to check construct validity. First, they make guesses (hypotheses) based on theory about how people with different levels of the trait should act or score. Then, they use or build a test that matches the trait. After collecting results, they check if the test outcomes agree with their expectations. If yes, it supports the test and the idea behind it. If not, the problem might be with the test or the theory itself (Mehrens & Lehmann, 1984).</p>
                <p>There are different ways to collect proof for construct validity (Crocker & Algina, 1986). One way is checking how scores relate to other things using correlations. Another way is comparing scores between different groups, like men and women, to see if the results match what the theory says. </p>                    
                <p>Factor analysis is also used to see if questions group in expected ways. Confirmatory Factor Analysis (CFA) is a widely employed statistical method used to evaluate whether the observed data align with a theoretically specified factor structure. In the context of construct validity, CFA provides robust evidence by testing whether observed variables reflect the intended latent constructs. The choice of estimation technique in confirmatory factor analysis depends largely on whether the data is categorical or continuous. When working with categorical data, researchers often employ the WLSMV estimator. In contrast, Maximum Likelihood (ML) is better suited for continuous variables. Each method offers a different approach to examining whether the proposed factor model is consistent with the structure reflected in the observed data (Rhemtulla et al., 2012; Xia & Yang, 2019).</p>
                <p>To interpret CFA results effectively, several fit indices are evaluated. The Chi-square statistic (χ²) tests exact model fit but is highly sensitive to sample size; thus, it is often complemented by approximate fit indices. The Comparative Fit Index (CFI) and Tucker-Lewis Index (TLI) are recommended to exceed 0.95 for good fit (Hu & Bentler, 1999). The Root Mean Square Error of Approximation (RMSEA) should ideally be below 0.06, with 90% confidence interval preferably not exceeding 0.08 (MacCallum et al., 1996). Additionally, the Standardized Root Mean Square Residual (SRMR) should remain below 0.08 (Kline, 2015). High standardized factor loadings—commonly above 0.70—are also indicative of strong relationships between indicators and latent variables (Tabachnick & Fidell, 2013), reinforcing convergent validity within the construct. </p>
                <p>Another method is the multitrait-multimethod matrix, which checks if the same trait is measured the same way with different tools (Mehrens & Lehmann, 1984).</p>
                <p>Construct validity doesn't depend on just one study. Instead, it grows stronger as more studies and methods agree over time. Some researchers also use generalizability theory to see if the results stay the same in different formats or settings. All of these help us feel sure that a test is really measuring what it claims to, and that the results are useful and understandable (Mehrens & Lehmann, 1984). </p>
                
                <b id="BC">Conclusion</b>
                <p>In this writing, we took a little journey through the history of validity!  Starting from the early 1900s, we saw how the meaning of validity slowly changed. It went from just checking if a test works to really thinking about what scores mean. Many smart people like Messick (1975), Zumbo (2005–2023), and Crocker & Algina (1986) showed us that test scores are more than numbers—they have to mean something in real life too!</p>
                <p>Now, we know that a good test isn’t just about getting answers right or wrong. It also needs to be fair and clear for everyone, no matter who they are or where they live. Validity is like a team project—you need ideas, facts, and care to make it all work together!  Thanks for reading all the way—I hope you learned something and had a bit of fun too! :)</p>

                <b id="BR">References</b>
                <p class="citation">Anastasi, A. (1950). The concept of validity in the interpretation of test scores. <i>Educational and Psychological Measurement</i>, 10, 67–78. https://doi.org/10.1177/001316445001000105 </p>
                <p class="citation">Angoff, W.H. (1988). Validity: An evolving concept. In: H. Wainer & H.I. Braun (Eds.), <i>Test validity</i> (pp. 19-32). Lawrence Erlbaum Associates.</p>
                <p class="citation">Bingham, W.V. (1937). <i>Aptitudes and aptitude testing</i>. Harper.</p>
                <p class="citation">Borsboom, D., Mellenbergh, G.J., & van Heerden, J. (2004). The concept of validity. <i>Psychological Review</i>, 111(4), 1061-1071. https://doi.org/10.1037/0033-295X.111.4.10 61 </p>
                <p class="citation">Borsboom, D., Cramer, A.O.J., Kievit, R.A., Scholten, A.Z., & Franić, S. (2009). The end of construct validity. In R.W. Lissitz (Ed.), <i>The concept of validity: Revisions, new directions, and applications</i> (pp. 135–170). IAP Information Age Publishing. </p>
                <p class="citation">Courtis, S.A. (1921). Report of the standardization committee. <i>Journal of Educational Research</i>, 4(1), 78–90.</p>
                <p class="citation">Crocker, L., & Algina, J. (1986). <i>Introduction to classical and modern test theory</i>. Holt, Rinehart and Winston, 6277 Sea Harbor Drive, Orlando, FL 32887.</p>
                <p class="citation">Cronbach, L.J., & Meehl, P.E. (1955). Construct validity in psychological tests. <i>Psychological Bulletin</i>, 52(4), 281–302. https://doi.org/10.1037/h0040957</p>
                <p class="citation">DeVellis, R. F., & Thorpe, C. T. (2021). <i>Scale development: Theory and applications.</i> Sage publications.</p>
                <p class="citation">Embretson S.E. (Whitely). (1983). Construct validity: Construct representation versus nomothetic span. <i>Psychological Bulletin</i>, 93(1), 179–197. https://doi.org/10.1037/0033- 2909.93.1.179 </p>
                <p class="citation">Embretson, S.E. (2007). Construct validity: A universal validity system or just another test evaluation procedure? <i>Educational Researcher</i>, 36(8), 449-455. https://doi.org/10.3102/ 0013189X07311600 </p>
                <p class="citation">Guilford, J.P. (1946). New standards for test evaluation. <i>Educational and Psychological Measurement</i>, 6(4), 427-438. https://doi.org/10.1177/001316444600600401 </p>
                <p class="citation">Haig, B.D. (1999). Construct validation and clinical assessment. <i>Behaviour Change</i>, 16, 64-73.</p>
                <p class="citation">Haig, B.D. (2014). Investigating the psychological world: <i>Scientific method in the behavioral sciences</i>. MIT Press.</p>
                <p class="citation">Hubley, A.M., & Zumbo, B.D. (1996). A dialectic on validity: Where we have been and where we are going. <i>The Journal of General Psychology</i>, 123(3), 207-215. https://doi.org/10.1 080/00221309.1996.9921273 </p>
                <p class="citation">Hull, C.L. (1935). The conflicting psychologies of learning: A way out. <i>Psychological Review</i>. 42(6), 491–516. https://doi.org/10.1037/h0058665 </p>
                <p class="citation">Kane, M. (1992). An argument-based approach to validity. <i>Psychological Bulletin</i>, 112(3), 527–535. https://doi.org/10.1037/0033-2909.112.3.527 </p>
                <p class="citation">Kane, M. (2006). Validation. In R. Brennan (Ed.) <i>Educational measurement</i> (4th ed., pp. 17- 64). American Council on Education and Praeger. </p>
                <p class="citation">Kline, R. B. (2015). <i>Principles and practice of structural equation modeling</i> (4th ed.). Guilford Press. </p>
                <p class="citation">Lennon, R.T. (1956). Assumptions Underlying the Use of Content Validity. <i>Educational and Psychological Measurement</i>, 16(3), 294-304. https://doi.org/10.1177/001316445601600 303 </p>
                <p class="citation">Loevinger, J. (1957). Objective tests as instruments of psychological theory. <i>Psychological Reports</i>, 3, 635-694 (Monograph Supp. 9).</p>
                <p class="citation">MacCallum, R. C., Browne, M. W., & Sugawara, H. M. (1996). Power analysis and determination of sample size for covariance structure modeling. <i>Psychological Methods</i>, 1(2), 130-149.</p>
                <p class="citation">Maddox, B., Zumbo, B.D. (2017). Observing testing situations: Validation as Jazz. In: B.D. Zumbo, A.M. Hubley (eds) <i>Understanding and investigating response processes in validation research</i>. Springer, Cham. https://doi.org/10.1007/978-3-319-56129-5_10 </p>
                <p class="citation">Mehrens, W. A . , and Lehmann, I. J. ( 1 984) . <i>Measurement and evaluation in education and psychology</i> (3rd ed.). New York: Holt, Rinehart, and Winston.</p>
                <p class="citation">Messick, S. (1975). The standard problem: Meaning and values in measurement and evaluation. <i>American Psychologist</i>, 30, 955- 966. </p>
                <p class="citation">Millman, J. (1979). Reliability and validity of criterion-referenced test scores. In: R. Traub (Ed.), <i>New directions for testing and measurement: Methodological developments.</i> Jossey-Bass.</p>
                <p class="citation">Rhemtulla, M., Brosseau-Liard, P. E., & Savalei, V. (2012). When can categorical variables be treated as continuous? A comparison of robust continuous and categorical SEM estimation methods under suboptimal conditions. <i>Psychological Methods</i>, 17, 354–373.</p>
                <p class="citation">Ronna C. Turner & Laurie Carlson (2003) Indexes of Item-Objective Congruence for Multidimensional Items, <i>International Journal of Testing</i>, 3:2, 163-171, DOI: 10.1207/S15327574IJT0302_5 </p>
                <p class="citation">Rulon, P.J. (1946). On the validity of educational tests. <i>Harvard Educational Review</i>, 16, 290- 296. </p>
                <p class="citation">Schaffner, K.F. (2020). A comparison of two neurobiological models of fear and anxiety: A “construct validity” application? Perspectives on Psychological Science, 15(5), 1214- 1227. https://doi.org/10.1177/1745691620920860</p>
                <p class="citation">Shear, B.R., Zumbo, B.D. (2014). What counts as evidence: A review of validity studies in educational and psychological measurement. In: Zumbo, B.D., Chan, E.K.H. (eds) <i>Validity and validation in social, behavioral, and health sciences</i> (pp. 91-111). Springer, Cham. https://doi.org/10.1007/978-3-319-07794-9_6 </p>
                <p class="citation">Tabachnick, B. G., & Fidell, L. S. (2013). <i>Using multivariate statistics</i> (6th ed.). Pearson.</p>
                <p class="citation">Turner, R. C., & Carlson, L. (2003). Indexes of item-objective congruence for multidimensional items. <i>International journal of testing</i>, 3(2), 163-171.</p>
                <p class="citation">Xia, Y., & Yang, Y. (2019). RMSEA, CFI, and TLI in structural equation modeling with ordered categorical data: The story they tell depends on the estimation methods. <i>Behavior research methods</i>, 51, 409-428.</p>
                <p class="citation">Watson, J.B. (1913). Psychology as the behaviorist views it. Psychological Review, 20(2), 158– 177. https://doi.org/10.1037/h0074428 </p>
                <p class="citation">Zumbo, B.D. (Ed.). (1998). <i>Validity theory and the methods used in validation: perspectives from the social and behavioral sciences</i>. In: Social Indicators Research: An International and Interdisciplinary Journal for Quality-of-Life Measurement, [Special volume], Vol. 45, Issues 1-3. Springer International Publishing. </p>
                <p class="citation">Zumbo, B.D. (2005, July). <i>Reflections on validity at the intersection of psychometrics, scaling, philosophy of inquiry, and language testing</i> [Samuel J. Messick Memorial Award Lecture]. LTRC, the 27th Language Testing Research Colloquium, Ottawa, Canada. </p>
                <p class="citation">Zumbo, B.D. (2007a). Validity: Foundational Issues and Statistical Methodology. In C.R. Rao & S. Sinharay (Eds.), <i>Handbook of statistics</i> (Vol. 26, pp. 45–79). Elsevier. </p>
                <p class="citation">Zumbo, B.D. (2009). Validity as contextualized and pragmatic explanation, and its implications for validation practice. In R.W. Lissitz (ed.) <i>The concept of validity: Revisions, new directions, and applications</i> (pp. 65–82). IAP Information Age Publishing., </p>
                <p class="citation">Zumbo, B.D. (2015). <i>Consequences, side effects and the ecology of testing: Keys to considering assessment “in vivo”</i> [Plenary address]. Annual Meeting of the Association for Educational Assessment – Europe (AEAEurope), Glasgow, Scotland. https://youtu.b e/0L6Lr2BzuSQ </p>
                <p class="citation">Zumbo, B.D. (2019). Foreword: Tensions, Intersectionality, and What Is on the Horizon for International Large-Scale Assessments in Education. In B. Maddox (Ed.), <i>International large-scale assessments in education: Insider research perspectives </i>(pp. xii–xiv). Bloomsbury Publishing. https://doi.org/10.5040/9781350023635 </p>
                <p class="citation">Zumbo, B.D. (2021). <i>A novel multimethod approach to investigate whether tests delivered at a test centre are concordant with those delivered remotely online</i> [Research Monograph]. UBC Psychometric Research Series, University of British Columbia. http://dx.doi.org/1 0.14288/1.0400581 </p>
                <p class="citation">Zumbo, B. D. (2023). A dialectic on validity: Explanation-focused and the many ways of being human. <i>International Journal of Assessment Tools in Education</i>, 10(Special Issue), 1-96.</p>
            </div>
        </div>
    </section>

    <script src="/js/jquery-3.3.1.min.js"></script>
    <script src="/js/popper.min.js"></script>
    <script src="/js/bootstrap.min.js"></script>
    <script src="/js/Headroom.js"></script>
    <script src="/js/jQuery.headroom.js"></script>
    <script src="/js/owl.carousel.min.js"></script>
    <script src="/js/theme-and-loader.js"></script>
    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>

</html>
